{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":80739,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":67848,"modelId":92960}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom tqdm import tqdm_notebook as tqdm\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T19:20:57.709984Z","iopub.execute_input":"2024-08-21T19:20:57.710407Z","iopub.status.idle":"2024-08-21T19:20:57.717353Z","shell.execute_reply.started":"2024-08-21T19:20:57.710368Z","shell.execute_reply":"2024-08-21T19:20:57.716142Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"class SkinLesionDataset(Dataset):\n    def __init__(self, root, metadata_csv, image_folder, transform=None):\n        self.metadata = pd.read_csv(root + metadata_csv)\n        self.image_folder = root + image_folder\n        self.transform = transform\n\n    def __len__(self):\n        return 20000#len(self.metadata)\n\n    def __getitem__(self, idx):\n        img_name = self.metadata.iloc[idx]['isic_id']\n        img_path = f\"{self.image_folder}/image/{img_name}.jpg\"\n        image = Image.open(img_path)\n\n        if self.transform:\n            image = self.transform(image)\n\n        # Extract tabular data\n        tabular_data = self.metadata.iloc[idx][['age_approx', 'tbp_lv_areaMM2']].values.astype(np.float32)\n\n        # Convert 'sex' to a numerical value: 1 for 'male', 0 for 'female'\n        sex = self.metadata.iloc[idx]['sex']\n        sex_numeric = 1.0 if sex == 'male' else 0.0\n        \n        tabular_data = np.nan_to_num(tabular_data, nan=0)\n\n        # Append the numeric 'sex' value to tabular data\n        tabular_data = np.append(tabular_data, sex_numeric)\n\n        # Convert to tensor\n        tabular_data = torch.tensor(tabular_data, dtype=torch.float32)\n\n        # Target label\n        label = torch.tensor(self.metadata.iloc[idx]['target'], dtype=torch.float32)\n        \n        return image, tabular_data, label\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T19:20:58.139643Z","iopub.execute_input":"2024-08-21T19:20:58.140037Z","iopub.status.idle":"2024-08-21T19:20:58.150627Z","shell.execute_reply.started":"2024-08-21T19:20:58.140003Z","shell.execute_reply":"2024-08-21T19:20:58.149560Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T19:20:58.557400Z","iopub.execute_input":"2024-08-21T19:20:58.557817Z","iopub.status.idle":"2024-08-21T19:20:58.564170Z","shell.execute_reply.started":"2024-08-21T19:20:58.557782Z","shell.execute_reply":"2024-08-21T19:20:58.562903Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\nclass CnnNetTabular(nn.Module):\n    def __init__(self):\n        super(CnnNetTabular, self).__init__()\n        \n        self.cnn = models.efficientnet_b0(pretrained=False)\n        self.cnn.load_state_dict(torch.load(\"/kaggle/input/efficent-net-b0/pytorch/default/1/efficientnet_b0.pth\"))\n\n        # Replace the final classification layer with an identity layer\n        self.cnn.classifier[1] = nn.Identity()\n\n        # Simple feedforward for tabular data\n        self.tabular_net = nn.Sequential(\n            nn.Linear(3, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n        )\n\n        # Final combined classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(1280 + 32, 128),  # Adjust the input size to match the concatenated feature size\n            nn.ReLU(),\n            nn.Linear(128, 3)  # Adjust to output the correct number of classes\n        )\n\n    def forward(self, image, tabular_data):\n        img_features = self.cnn(image)  # Features from the CNN\n        tab_features = self.tabular_net(tabular_data)  # Features from tabular data\n        combined_features = torch.cat((img_features, tab_features), dim=1)\n        output = self.classifier(combined_features)\n        return output\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T19:20:59.019055Z","iopub.execute_input":"2024-08-21T19:20:59.019412Z","iopub.status.idle":"2024-08-21T19:20:59.030881Z","shell.execute_reply.started":"2024-08-21T19:20:59.019384Z","shell.execute_reply":"2024-08-21T19:20:59.029708Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"root = '/kaggle/input/isic-2024-challenge/'\n\n# Initialize dataset and dataloader\ndataset = SkinLesionDataset(root=root, metadata_csv='train-metadata.csv', image_folder='train-image', transform=transform)\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n\n# Initialize model, loss function, and optimizer\nmodel = CnnNetTabular().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(2):  # Number of epochs\n    model.train()\n    running_loss = 0.0\n    loss_through_training = []\n    i = 0\n    for images, tabular_data, labels in tqdm(dataloader):\n        \n        if (i % 100) == 0.:\n            print(\"batch: \", i)\n        i += 1\n        images = images.to(device)\n        tabular_data = tabular_data.to(device)\n        \n        labels = labels.to(device)\n        labels=labels.to(torch.int64)\n        optimizer.zero_grad()\n        outputs = model(images, tabular_data)\n        #print(outputs [0,:])\n       \n        loss = criterion(outputs.squeeze(), labels)\n        loss.backward()\n        #print(loss)\n        optimizer.step()\n        running_loss += loss.item()\n        loss_through_training.append(running_loss)\n        \n        if torch.isnan(images).any() or torch.isnan(tabular_data).any():\n            print(\"NaN detected in input data!\")\n            break\n\n        if torch.isinf(images).any() or torch.isinf(tabular_data).any():\n            print(\"Inf detected in input data!\")\n            break\n        \n        #print(outputs.size)\n        if torch.isnan(loss).any():\n            print(\"Warning: Loss has become NaN!\")\n            break\n       \n\n    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T19:27:47.574384Z","iopub.execute_input":"2024-08-21T19:27:47.575467Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/4208978846.py:3: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n  self.metadata = pd.read_csv(root + metadata_csv)\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n/tmp/ipykernel_34/734145247.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for images, tabular_data, labels in tqdm(dataloader):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c0e215d907d4a6683b7da37027a6cdb"}},"metadata":{}},{"name":"stdout","text":"batch:  0\nbatch:  100\nbatch:  200\nbatch:  300\nEpoch 1, Loss: 0.023130282514049843\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/734145247.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for images, tabular_data, labels in tqdm(dataloader):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2660143d5d642f38866c9915ba6b7f0"}},"metadata":{}},{"name":"stdout","text":"batch:  0\nbatch:  100\nbatch:  200\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:31:32.663404Z","iopub.execute_input":"2024-08-21T17:31:32.663848Z","iopub.status.idle":"2024-08-21T17:31:32.669258Z","shell.execute_reply.started":"2024-08-21T17:31:32.663806Z","shell.execute_reply":"2024-08-21T17:31:32.668218Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:37:30.514888Z","iopub.execute_input":"2024-08-21T17:37:30.515303Z","iopub.status.idle":"2024-08-21T17:37:30.524356Z","shell.execute_reply.started":"2024-08-21T17:37:30.515267Z","shell.execute_reply":"2024-08-21T17:37:30.522452Z"},"trusted":true},"execution_count":136,"outputs":[{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"0.0006619761402362201"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}